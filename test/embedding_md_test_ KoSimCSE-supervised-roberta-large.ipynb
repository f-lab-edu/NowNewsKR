{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model 의 문장 유사도 테스트\n",
    "### [KoSimCSE-supervised-roberta-large](https://huggingface.co/daekeun-ml/KoSimCSE-supervised-roberta-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/embedding_test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 582/582 [00:00<00:00, 1.06MB/s]\n",
      "model.safetensors: 100%|██████████| 1.35G/1.35G [01:04<00:00, 20.9MB/s]\n",
      "config.json: 100%|██████████| 547/547 [00:00<00:00, 1.35MB/s]\n",
      "model.safetensors: 100%|██████████| 1.35G/1.35G [00:24<00:00, 54.2MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100%|██████████| 415/415 [00:00<00:00, 561kB/s]\n",
      "vocab.txt: 100%|██████████| 248k/248k [00:00<00:00, 507kB/s]\n",
      "tokenizer.json: 100%|██████████| 752k/752k [00:00<00:00, 998kB/s] \n",
      "special_tokens_map.json: 100%|██████████| 173/173 [00:00<00:00, 797kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[92.9861]], grad_fn=<MulBackward0>) tensor([[83.0575]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from transformers import AutoConfig, PretrainedConfig, PreTrainedModel\n",
    "from transformers import AutoModel, AutoTokenizer, logging\n",
    "\n",
    "class SimCSEConfig(PretrainedConfig):\n",
    "    def __init__(self, version=1.0, **kwargs):\n",
    "        self.version = version\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class SimCSEModel(PreTrainedModel):\n",
    "    config_class = SimCSEConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_pretrained(config.base_model)\n",
    "        self.hidden_size: int = self.backbone.config.hidden_size\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor,\n",
    "        attention_mask: Tensor = None,\n",
    "        # RoBERTa variants don't have token_type_ids, so this argument is optional\n",
    "        token_type_ids: Tensor = None,\n",
    "    ) -> Tensor:\n",
    "        # shape of input_ids: (batch_size, seq_len)\n",
    "        # shape of attention_mask: (batch_size, seq_len)\n",
    "        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        emb = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        if self.training:\n",
    "            emb = self.dense(emb)\n",
    "            emb = self.activation(emb)\n",
    "\n",
    "        return emb\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SimCSEModel.from_pretrained(\"daekeun-ml/KoSimCSE-supervised-roberta-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"daekeun-ml/KoSimCSE-supervised-roberta-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_embedding_score(tokenizer, model, sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    embeddings = model(**inputs)\n",
    "    score01 = cal_score(embeddings[0,:], embeddings[1,:])\n",
    "    score02 = cal_score(embeddings[0,:], embeddings[2,:])\n",
    "    score03 = cal_score(embeddings[1,:], embeddings[2,:])\n",
    "    \n",
    "    score = [score01, score02, score03 ]\n",
    "    max_score = max(score).item()\n",
    "    max_score_idx = score.index(max(score))\n",
    "    \n",
    "    if max_score_idx == 0:\n",
    "        print(f\"1,2번 문장이 {max_score}로 가장 유사합니다.\")\n",
    "    elif max_score_idx == 1:  \n",
    "        print(f\"1,3번 문장이 {max_score}로 가장 유사합니다.\")\n",
    "    else:    \n",
    "        print(f\"2,3번 문장이 {max_score}로 가장 유사합니다.\") \n",
    "\n",
    "    print('(', score01.item(), score02.item(), score03.item(), ')')\n",
    "\n",
    "def cal_score(a, b):\n",
    "    if len(a.shape) == 1: a = a.unsqueeze(0)\n",
    "    if len(b.shape) == 1: b = b.unsqueeze(0)\n",
    "    a_norm = a / a.norm(dim=1)[:, None]\n",
    "    b_norm = b / b.norm(dim=1)[:, None]\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1)) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2번 문장이 92.98614501953125로 가장 유사합니다.\n",
      "( 92.98614501953125 83.0574722290039 77.81117248535156 )\n"
     ]
    }
   ],
   "source": [
    "# Inference example\n",
    "sentences = ['이번 주 일요일에 분당 이마트 점은 문을 여나요?',\n",
    "             '일요일에 분당 이마트는 문 열어요?',\n",
    "             '분당 이마트 점은 토요일에 몇 시까지 하나요']\n",
    "\n",
    "show_embedding_score(tokenizer, model, sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## txt 파일로 저장되어있는 query set에 대한 유사도 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['치타가 들판을 가로 질러 먹이를 쫓는다.', '치타 한 마리가 먹이 뒤에서 달리고 있다.', '원숭이 한 마리가 드럼을 연주한다.'],\n",
       " ['이번 주 일요일에 분당 이마트 점은 문을 여나요?',\n",
       "  '일요일에 분당 이마트는 문 열어요?',\n",
       "  '분당 이마트 점은 토요일에 몇 시까지 하나요?'],\n",
       " ['빠른 갈색 여우가 게으른 개를 뛰어넘는다', '날쌘 갈색 여우가 졸린 개를 넘어간다', '태양은 동쪽에서 뜨고 서쪽에서 진다'],\n",
       " ['인공지능은 세계를 변화시키고 있다', 'AI는 우리 행성을 혁신하고 있다', '사과는 맛있고 영양가 있는 과일이다'],\n",
       " ['파이썬은 인기 있는 프로그래밍 언어이다',\n",
       "  '많은 개발자들이 소프트웨어 개발을 위해 파이썬을 사용한다',\n",
       "  '코끼리는 가장 큰 육상 동물이다'],\n",
       " ['커피는 인기 있는 음료이다', '많은 사람들이 아침에 커피를 마신다', '산에는 여러 종류의 나무가 있다'],\n",
       " ['데이터 과학은 정보를 분석하는 분야이다',\n",
       "  '많은 회사들이 데이터 분석을 위해 데이터 과학자를 고용한다',\n",
       "  '바다는 광대하고 신비로운 곳이다'],\n",
       " ['서울은 대한민국의 수도이다', '서울에는 다양한 문화와 역사가 존재한다', '사막은 건조하고 모래가 많은 지역이다'],\n",
       " ['성남에는 편의점이 몇 개가 있을까?', '너가 자주 가는 편의점은 성남에 있니?', '서울은 편의시설이 얼마나 많은가?'],\n",
       " ['컴퓨터 프로그래밍은 창의적인 과정이다',\n",
       "  '많은 사람들이 소프트웨어 개발을 배운다',\n",
       "  '산책하며 신선한 공기를 마시는 것은 건강에 좋다'],\n",
       " ['수학은 문제 해결에 중요하다', '공학 분야에서 수학은 필수적이다', '영화 감상은 좋은 여가 활동이다'],\n",
       " ['건강한 식단은 삶의 질을 향상시킨다', '많은 사람들이 영양가 있는 음식을 찾는다', '별을 관찰하는 것은 매력적인 취미이다'],\n",
       " ['운동은 몸과 마음에 좋다', '규칙적인 운동은 건강을 유지하는데 도움이 된다', '좋은 책을 읽는 것은 지식을 넓히는데 도움이 된다'],\n",
       " ['여행은 새로운 경험을 제공한다',\n",
       "  '많은 사람들이 문화적 다양성을 경험하기 위해 여행한다',\n",
       "  '좋은 음악을 듣는 것은 마음을 진정시킨다'],\n",
       " ['언어 학습은 두뇌에 도전을 준다', '외국어를 배우는 것은 커뮤니케이션 능력을 향상시킨다', '가드닝은 평화롭고 만족스러운 활동이다'],\n",
       " ['영화 제작은 창의력을 요구하는 예술이다',\n",
       "  '감독과 배우들은 영화의 이야기를 만든다',\n",
       "  '맛있는 음식을 요리하는 것은 재미있는 활동이다']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "with open('query.txt', 'r') as file:\n",
    "    \n",
    "    file_contents = file.read()\n",
    "    query_list = literal_eval(file_contents)\n",
    "\n",
    "query_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0)\n",
      "['치타가 들판을 가로 질러 먹이를 쫓는다.', '치타 한 마리가 먹이 뒤에서 달리고 있다.', '원숭이 한 마리가 드럼을 연주한다.']\n",
      "1,2번 문장이 83.16889953613281로 가장 유사합니다.\n",
      "( 83.16889953613281 24.935367584228516 27.913841247558594 )\n",
      "\n",
      "1)\n",
      "['이번 주 일요일에 분당 이마트 점은 문을 여나요?', '일요일에 분당 이마트는 문 열어요?', '분당 이마트 점은 토요일에 몇 시까지 하나요?']\n",
      "1,2번 문장이 92.98614501953125로 가장 유사합니다.\n",
      "( 92.98614501953125 83.68720245361328 78.30581665039062 )\n",
      "\n",
      "2)\n",
      "['빠른 갈색 여우가 게으른 개를 뛰어넘는다', '날쌘 갈색 여우가 졸린 개를 넘어간다', '태양은 동쪽에서 뜨고 서쪽에서 진다']\n",
      "1,2번 문장이 76.48316955566406로 가장 유사합니다.\n",
      "( 76.48316955566406 32.25828170776367 31.092655181884766 )\n",
      "\n",
      "3)\n",
      "['인공지능은 세계를 변화시키고 있다', 'AI는 우리 행성을 혁신하고 있다', '사과는 맛있고 영양가 있는 과일이다']\n",
      "1,2번 문장이 87.18400573730469로 가장 유사합니다.\n",
      "( 87.18400573730469 36.52079772949219 38.64492416381836 )\n",
      "\n",
      "4)\n",
      "['파이썬은 인기 있는 프로그래밍 언어이다', '많은 개발자들이 소프트웨어 개발을 위해 파이썬을 사용한다', '코끼리는 가장 큰 육상 동물이다']\n",
      "1,2번 문장이 90.1249771118164로 가장 유사합니다.\n",
      "( 90.1249771118164 32.72272491455078 28.334888458251953 )\n",
      "\n",
      "5)\n",
      "['커피는 인기 있는 음료이다', '많은 사람들이 아침에 커피를 마신다', '산에는 여러 종류의 나무가 있다']\n",
      "1,2번 문장이 69.57415008544922로 가장 유사합니다.\n",
      "( 69.57415008544922 39.17942810058594 32.390201568603516 )\n",
      "\n",
      "6)\n",
      "['데이터 과학은 정보를 분석하는 분야이다', '많은 회사들이 데이터 분석을 위해 데이터 과학자를 고용한다', '바다는 광대하고 신비로운 곳이다']\n",
      "1,2번 문장이 81.42179107666016로 가장 유사합니다.\n",
      "( 81.42179107666016 40.40068817138672 36.444766998291016 )\n",
      "\n",
      "7)\n",
      "['서울은 대한민국의 수도이다', '서울에는 다양한 문화와 역사가 존재한다', '사막은 건조하고 모래가 많은 지역이다']\n",
      "1,2번 문장이 74.34449005126953로 가장 유사합니다.\n",
      "( 74.34449005126953 18.288894653320312 27.266719818115234 )\n",
      "\n",
      "8)\n",
      "['성남에는 편의점이 몇 개가 있을까?', '너가 자주 가는 편의점은 성남에 있니?', '서울은 편의시설이 얼마나 많은가?']\n",
      "1,2번 문장이 88.50782775878906로 가장 유사합니다.\n",
      "( 88.50782775878906 59.857154846191406 55.172157287597656 )\n",
      "\n",
      "9)\n",
      "['컴퓨터 프로그래밍은 창의적인 과정이다', '많은 사람들이 소프트웨어 개발을 배운다', '산책하며 신선한 공기를 마시는 것은 건강에 좋다']\n",
      "1,2번 문장이 69.05506134033203로 가장 유사합니다.\n",
      "( 69.05506134033203 30.997270584106445 30.70113754272461 )\n",
      "\n",
      "10)\n",
      "['수학은 문제 해결에 중요하다', '공학 분야에서 수학은 필수적이다', '영화 감상은 좋은 여가 활동이다']\n",
      "1,2번 문장이 85.31087493896484로 가장 유사합니다.\n",
      "( 85.31087493896484 37.4764518737793 32.711124420166016 )\n",
      "\n",
      "11)\n",
      "['건강한 식단은 삶의 질을 향상시킨다', '많은 사람들이 영양가 있는 음식을 찾는다', '별을 관찰하는 것은 매력적인 취미이다']\n",
      "1,2번 문장이 67.08512115478516로 가장 유사합니다.\n",
      "( 67.08512115478516 35.33770751953125 32.194053649902344 )\n",
      "\n",
      "12)\n",
      "['운동은 몸과 마음에 좋다', '규칙적인 운동은 건강을 유지하는데 도움이 된다', '좋은 책을 읽는 것은 지식을 넓히는데 도움이 된다']\n",
      "1,2번 문장이 89.25023651123047로 가장 유사합니다.\n",
      "( 89.25023651123047 50.764991760253906 48.830562591552734 )\n",
      "\n",
      "13)\n",
      "['여행은 새로운 경험을 제공한다', '많은 사람들이 문화적 다양성을 경험하기 위해 여행한다', '좋은 음악을 듣는 것은 마음을 진정시킨다']\n",
      "1,2번 문장이 72.86495208740234로 가장 유사합니다.\n",
      "( 72.86495208740234 49.6871223449707 43.533878326416016 )\n",
      "\n",
      "14)\n",
      "['언어 학습은 두뇌에 도전을 준다', '외국어를 배우는 것은 커뮤니케이션 능력을 향상시킨다', '가드닝은 평화롭고 만족스러운 활동이다']\n",
      "1,2번 문장이 76.27630615234375로 가장 유사합니다.\n",
      "( 76.27630615234375 27.571767807006836 35.93702697753906 )\n",
      "\n",
      "15)\n",
      "['영화 제작은 창의력을 요구하는 예술이다', '감독과 배우들은 영화의 이야기를 만든다', '맛있는 음식을 요리하는 것은 재미있는 활동이다']\n",
      "1,2번 문장이 77.45624542236328로 가장 유사합니다.\n",
      "( 77.45624542236328 48.709964752197266 35.08404541015625 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, query in enumerate(query_list):\n",
    "    print(f'{i})\\n{query}')\n",
    "    show_embedding_score(tokenizer, model, query)\n",
    "    print() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedding_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
